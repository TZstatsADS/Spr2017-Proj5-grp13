---
title: "R Notebook"
output: html_notebook
---

Run the eda.Rmd first
```{r}
library(caret)
library(glmnet)
library(leaps)
library(randomForest)
library(gbm)
```


### Prediction Part

Goal:  
Now that we understand the characteristics of the dataset and the significance of explanatory variables through plots and regression analysis, we now move on to trying to predict the probability of a given patient to not show up. The motivation behind this is 2-fold:  
  
1) If we know that a patient is very likely to not show-up given some characteristics at the very point he/she books an appointment, we can incentivise the patient to show-up or to perhaps redirect the patient to a certain part of the day to facilitate handling no-shows on an aggregate level.  
  
2) If we know the number of patients who are very likely to not show up, we can come up with several strategies which are similar to overbooking so as to increase utilization of resources.   
  
**Feature Creation**   
```{r}
# This feature is created because of the graph: "Percentage of No-Shows depending on the waiting days" in the Exploratory Data Analysis
dataset$daydiff_regist_appt_squared <- dataset$daydiff_regist_appt**2
```
  
**Seperation Train/Test set**  
```{r}
intrain <- createDataPartition(y=dataset$show_up, p=0.7, list=FALSE)
df.train <- dataset[intrain, ]
df.test <- dataset[-intrain, ]
```
  
**Imbalanced Classes**  
  
Our dataset is imbalanced and we need to keep that in mind when trying to predict the number of no-shows. This imbalanceness is totally normal and was expected (most patient do show-up and that's perfectly normal). However, having an imbalanced dataset will make the model comparison task more difficult.    
  
```{r}
sum(dataset$show_up==1)/nrow(dataset)*100
```
  
69.70% of the patients do show up. As a result, predicting that all patient will show-up will guve an accuracy of 69.70%. Obviously this model is not satisfactory.      
  
Hence accuracy may not be the best metric to focus on when comparing a model. (See Accuracy Paradox (https://en.wikipedia.org/wiki/Accuracy_paradox)).    
  
Few metrics or methods we can use are the following:  
- Recall  
- Precision  
- ROC Curve  
  
**Feature Selection**  
  
In this section, we will try to discover which features are the most important. Hence, we will be able to verify if we get the same conclusions as in the exploratory data analysis.   
  
```{r}
# Select Columns
df.train.sub = df.train[,-c(1,8,19,20,21,23,24,25,26,31)]
df.test.sub = df.test[,-c(1,8,19,20,21,23,24,25,26,31)]

# Feature Selection
fit = regsubsets(show_up~., 
                 data = df.train.sub,
                 method = "exhaustive", 
                 nvmax = 20)
plot(fit)
```
  
Important variables (by decreasing order):   
- age  
- daydiff_regist_appt  
- daydiff_regist_appt_squared  
- hour_registration  
- smoker1  
- scholarship1  
- alcoholism1  
- sms_reminder1  
- appointment_Mon  
- appointment_Sat  
Those was the one having the more impact in the Exploratory Data Analysis. Those results aren't a surprise.  
   
```{r}
plot(summary(fit)$adjr2)
```
  
```{r}
plot(summary(fit)$cp)
```
  
```{r}
plot(summary(fit)$bic)
```
  
We infer from those graphs that there is no optimum number of variables below 15 using regressions; in addition, the 9 first variables listed above seem to extract most of what can be extracted using linear regression. It is not a surprising result considering our previous analysis and the fact that most of our features are factor variables.   
    
However, this analysis straighten some of our previous points: age, daydiff_regist_appt and hour_registration are important features that explain a lot the show/noshow ratio.     
    
Let us now focus on the predictive model. Apparently, after this preliminary analysis, linear correlation don't explain enough to use it as a predictive model. Thus, let us focus on tree-based algorithm.    
  
**Tree-base method**   
  
```{r}
set.seed(23)
fit.tree = randomForest(show_up~., 
                        data = df.train.sub, 
                        importance=TRUE, 
                        mtry = 2, 
                        ntree=100)
fit.tree
```

```{r}
importance(fit.tree)[,4]
```
    
The more important variables are the same as the on highlighted above:  
- Age  
- Hour of registration  
- The number of days between the registration and the appointment  
   
Let's look at this model on the test set:   
```{r}
rf.pred <- predict(fit.tree, 
                   df.test.sub, 
                   type="class")
table(rf.pred, df.test.sub$show_up)
1-mean(rf.pred==df.test.sub$show_up)
```
The test error rate is 29.8%.  Looking at the confusion matrix, two third of the predicted No-Shows are correctly predictedbut overall we predict only 1,000 No-Shows while there are around ~26,000 No-Shows to predict.     
    
At this point we need to wonder which is more important - detecting all no-shows at a lower confidence or detecting less no-shows but at a higher confidence? We will return to this question later.   
     
We now try to find the optimal parameters for the Random Forest model (note that computer might crash with too many trees):    
```{r} 
rf.fit <- randomForest(show_up~., 
                        data = df.train.sub, 
                        importance=TRUE, 
                        mtry = 2, 
                        ntree = 1000)
```
  
```{r}
err <- rf.fit$err.rate[,]
nb_trees <- seq(1:1000)
err <- as.data.frame(cbind(err, nb_trees))

ggplot(err, aes(x=nb_trees, y=OOB)) +
  geom_line() +
  ggtitle("Out-Of-Bag Error depending on the number of trees")
```
   
The out-of-bag error is a good approximation of the out of sample event. It allows us to not use cross-validation which necessites a huge computation power.  
We see that augmenting the number of trees isn't helping much. The out-of-bag error is bounded to ~29%.    
   
We can also try to use more sub-trees.    
```{r}
set.seed(23)
fit.tree = randomForest(show_up~., 
                        data = df.train.sub, 
                        importance=TRUE, 
                        mtry = 3, 
                        ntree=100)
fit.tree
```
Increasing the number of splits increases the out-of-bag error which means is it will lead to overfitting.     
    
**Boosting**    
Given that the predictions are not very different.   
```{r}
boost.fit <- gbm(show_up~., 
                 data=df.train.sub,
                 distribution="adaboost",
                 n.trees=100,
                 interaction.depth=4,
                 shrinkage=0.01)

boost.fit
summary(boost.fit)
```

```{r}
boost.fit$train.error[100]
```
The training error is very high.    
    
Prediction on the test set.    
```{r}
boost.probs <- predict(boost.fit, 
                       df.test.sub, 
                       type="response",
                       n.trees=100)
boost.pred <- rep(0,nrow(df.test.sub))
boost.pred[boost.probs >.5] <- 1
table(boost.pred, df.test.sub$show_up)
1-mean(boost.pred==df.test.sub$show_up)
```

The boosting model doesn't help. It predicts 1 every time because the predicted probabilities are very high. We can try to change the threshold.       
   
Now we try to optimize the threshold to select for high specificity/true negativity rate - ultimately, we want a high confidence of correctly predicting the number of no-shows for any given day.    
Hence, we are not looking at accuracy but at specificity.    
  
```{r}
boost.ThresholdsList <- seq(from=min(log(boost.probs)), 
                            to=max(log(boost.probs)), 
                            by=0.001)
boost.specificityList <- c()

for (i in 1:length(boost.ThresholdsList)){
  threshold <- boost.ThresholdsList[i]
  
  pred.boost <- factor(ifelse(log(boost.probs) > threshold, 1, 0))
  boost.mat <- confusionMatrix(pred.boost, df.test$show_up)
  
  true_neg <- boost.mat$table[1]
  false_pos <- boost.mat$table[3]
  spec <- true_neg / (true_neg + false_pos)
  
  boost.specificityList[i] <- spec
}

boost.thresholdsDF <- data.frame("ThresholdsList"=boost.ThresholdsList,
                                 "Boost.specificity"=boost.specificityList)

ggplot(boost.thresholdsDF, aes(x=ThresholdsList, y=Boost.specificity)) +
  geom_line() +
  ggtitle("Out-Of-Bag Error depending on the number of trees")
```

Some threshods give very good values of specificity. However, each threshold below 0.07 is able to predict less than 50 No-Shows out of ~26,000 No-Shows which is not helpful.    
 

### Business Value
To evaluate the value of building an effective prediction model, we can look at use cases at the individual and day-aggregated level.

At the individual level, we can use a prediction model which only includes features that is fully known at the point of appointment booking (no weather in this case) to categorize if a patient is considered higher-risk above a certain threshold. This allows us to do two things:
1. Focus resources on providing more interventions (even more sms and calls) to increase likelihood of patient showing up. Or to confirm if patient is not going to show-up 2 days before appointment.
2. Give dollar incentives to channel likely no-shows into morning/evening appointment slots so that most of the no-shows are clustered around a certain period of the day. 

At the day-aggregated level, we can use a prediction model which INCLUDES weather features given by meteorological forecasts, that is able to give us the number of no-shows for a given day at a 95% confidence level. This can be computed 2 days before the actual appointment date.   

All this information from above can help us implement a "pseudo-overbooking" policy: an opt-in waitlist. The opt-in waitlist can be generated for each appointment day, 2 days in advance and the length of the list = # people who rescheduled + (conservative) estimate of # predicted no-shows. Given that we have "consolidated" in advance the likely no-shows into a certain part of the day, we can ask the wait-listers to come at a specific window if they are willing to take the risk to have an earlier appointment.  
  
Even if we are only able to cut the number of no-shows from 30% to 25%, we can already generate an estimated savings of $0.5 million for a community hospital. The savings will be even greater for a larger hospital. Furthermore, we can generate customer satisfaction amongst those wait-listers who manage to get an earlier appointment. 
