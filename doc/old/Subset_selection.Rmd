---
title: "R Notebook"
output: html_notebook
---


## Predictive model

### Feature selection

```{r}
load("~/Desktop/Spr2017-Proj5-grp13-master/output/with_weather_dataset.RData")
dataset$lowest_visibility_miles = as.numeric(dataset$lowest_visibility_miles)

library(glmnet)
library(leaps)
library(randomForest)
```

Subset selection : run exhaustive search to validate our previous results:

#### Using bic

```{r}
X = dataset[,-c(1,8,19,20,21,23,24,25,26,31)]

X$daydiff_regist_appt_squared = X$daydiff_regist_appt**2


fit = regsubsets(show_up~.,data = X,method = "exhaustive", nvmax = 20)
plot(fit)
```

Important variables (by importance order): 
- age
- daydiff_regist_appt
- daydiff_regist_appt_squared
- hour_registration
- smoker1
- scholarship1
- alcoholism1
- sms_reminder1
- appointment_Mon
- appointment_Sat

```{r}
plot(summary(fit)$adjr2)
```

```{r}
plot(summary(fit)$cp)
```

```{r}
plot(summary(fit)$bic)
```

We infer from those graphs that there is no optimum number of variables below 15 using regressions; in addition, the 9 first variables listed above seem to extract most of what can be extracted using linear regression. It is not a surprising result considering our previous analysis and the fact that most of our features are factor variables. 

However, this analysis straighten some of our previous points: age, daydiff_regist_appt and hour_registration are important features that explain a lot the show/noshow ratio. 

Let us now focus on the predictive model. Apparently, after this preliminary analysis, linear correlation don't explain enough to use it as a predictive model. Thus, let us focus on tree-based algorithm.


## Predictive model

### Tree-based

```{r}
fit.tree = randomForest(show_up~., data = X, importance=TRUE,mtry = 2, ntree=100)
fit.tree
```

```{r}
importance(fit.tree)[,4]
```

Let us run the same random forest using only the variables of interest and the most important using the tree above with more trees:
- age
- daydiff_regist_appt
- daydiff_regist_appt_squared
- hour_registration
+ 
- day_of_year_appointment
- day_appointment
- weekday_appointment
- avg_temp_faren
- max_wind_mph 

```{r}
X = X[,c(2,5,7,16,17,18,19,21,22,24)]
```

```{r}
fit.tree2 = randomForest(show_up~., data = X, ntree=100)
fit.tree2
```


### Regression-based

```{r}
fit.glm = glm(show_up~., data = X, family = "binomial")
```

```{r}
table(as.integer(predict(fit.glm)>0.5),X$show_up)
```

```{r}
L = c()
for(i in c(0,0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)){
  L = c(L,sum(diag(table(as.integer(predict(fit.glm)>i),X$show_up)))/sum(table(as.integer(predict(fit.glm)>i),X$show_up)))
}
plot(c(0,0.01,0.02,0.03,0.04,0.05,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9),L)
```

