---
title: "R Notebook"
output: html_notebook
---

Run the eda.Rmd first
```{r}
library(caret)
library(glmnet)
library(leaps)
library(randomForest)
```


### Prediction Part

Goal:  
[Brief description to write]  

**Seperation Train/Test set**
```{r}
intrain <- createDataPartition(y=dataset$show_up, p=0.7, list=FALSE)
df.train <- dataset[intrain, ]
df.test <- dataset[-intrain, ]
```

**Imbalanced Classes**

Our dataset is imbalanced and we need to keep that in mind when trying to predict the number of no-shows. This imbalanceness is totally normal and was ecpected (most patient do show-up and that's perfectly normal). However, having an imbalanced dataset will make the model comparison task more difficult.  

```{r}
sum(dataset$show_up==1)/nrow(dataset)*100
```

69.70% of the patients do show up. As a result, predicting that all patient will show-up will guve an accuracy of 69.70%. Obviously this model is not satisfactory.    

Hence accuracy may not be the best metric to focus on when comparing a model. (See Accuracy Paradox (https://en.wikipedia.org/wiki/Accuracy_paradox)).  

Few metrics or methods we can use are the following:  
- Recall  
- Precision  
- ROC Curve  

**Feature Selection**

In this section, we will try to discover which features are the most important. Hence, we will be able to verify if we get the same conclusions as in the exploratory data analysis.  

```{r}
# Select Columns
df.train.sub = df.train[,-c(1,8,19,20,21,23,24,25,26,31)]
df.test.sub = df.test[,-c(1,8,19,20,21,23,24,25,26,31)]

# Create Column 
# This feture is created because of the graph: "Percentage of No-Shows depending on the waiting days" in the Exploratory Data Analysis
df.train.sub$daydiff_regist_appt_squared = df.train.sub$daydiff_regist_appt**2

fit = regsubsets(show_up~., 
                 data = df.train.sub,
                 method = "exhaustive", 
                 nvmax = 20)
plot(fit)
```

Important variables (by decreasing order):   
- age  
- daydiff_regist_appt  
- daydiff_regist_appt_squared  
- hour_registration  
- smoker1  
- scholarship1  
- alcoholism1  
- sms_reminder1  
- appointment_Mon  
- appointment_Sat  
Those was the one having the more impact in the Exploratory Data Analysis. Those results aren't a surprise.  

```{r}
plot(summary(fit)$adjr2)
```

```{r}
plot(summary(fit)$cp)
```

```{r}
plot(summary(fit)$bic)
```

We infer from those graphs that there is no optimum number of variables below 15 using regressions; in addition, the 9 first variables listed above seem to extract most of what can be extracted using linear regression. It is not a surprising result considering our previous analysis and the fact that most of our features are factor variables.   
  
However, this analysis straighten some of our previous points: age, daydiff_regist_appt and hour_registration are important features that explain a lot the show/noshow ratio.   
  
Let us now focus on the predictive model. Apparently, after this preliminary analysis, linear correlation don't explain enough to use it as a predictive model. Thus, let us focus on tree-based algorithm.  
  
**Tree-base method**

```{r}
fit.tree = randomForest(show_up~., 
                        data = df.train.sub, 
                        importance=TRUE, 
                        mtry = 2, 
                        ntree=100)
fit.tree
```

```{r}
importance(fit.tree)[,4]
```

The more important variables are the same as the on highlighted above:  
- Age  
- Hour of registration  
- The number of days between the registration adn the appointment  
  
Let's look at this model on the test set: 
```{r}
rf.pred <- predict(fit.tree, 
                   df.train.sub, 
                   type="class")
table(rf.pred, df.train$show_up)
1-mean(rf.pred==df.train$show_up)
```
The test error rate is 27%.  Looking at the confusion matrix, we spot most of the No-Show but we also classify 53,956 appoitnment as potential No-Show while it is not the case.  
At this point we need to wonder what is the more important between detecting all no-shows & detecting only real no-shows (but not being able of detecting them all).

```{r}
rf.fit <- randomForest(show_up~., 
                        data = df.train.sub, 
                        importance=TRUE, 
                        mtry = 2, 
                        ntree=1000)
```

```{r}
rf.pred <- predict(rf.fit, 
                   df.train.sub, 
                   type="class")
table(rf.pred, df.train$show_up)
1-mean(rf.pred==df.train$show_up)
```

```{r}
err <- rf.fit$err.rate
```

